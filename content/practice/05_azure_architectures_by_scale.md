---
title: "Azure AI Architectures by Scale ‚Äì AI Application & AI ML/DL"
description: "Thi·∫øt k·∫ø h·ªá th·ªëng cho AI Application (LLM API) v√† AI Machine Learning/Deep Learning t·ª´ ngh√¨n ƒë·∫øn tri·ªáu request tr√™n Azure."
category: architecture
---

# Azure AI Architectures by Scale ‚Äì AI Application & AI ML/DL

T√†i li·ªáu thi·∫øt k·∫ø **h·ªá th·ªëng backend AI** tr√™n Azure theo **thang request** (ngh√¨n ‚Üí tri·ªáu), chia th√†nh **hai nh√°nh**:

1. **AI Application** ‚Äì ·ª®ng d·ª•ng g·ªçi LLM qua API (RAG, chatbot, agent); d√πng Azure OpenAI, Azure AI Search, Azure AI Foundry‚Ä¶
2. **AI Machine Learning / Deep Learning** ‚Äì H·ªá th·ªëng train & serve model (classification, CV, NLP custom); d√πng Azure ML, batch/online inference, Feature Store‚Ä¶

M·ªói nh√°nh c√≥ **nhi·ªÅu ki·∫øn tr√∫c** t∆∞∆°ng ·ª©ng **Tier 1 ‚Üí Tier 4** (1K ‚Äì 10K ‚Üí 500K ‚Äì 2M+ request/ng√†y), v·ªõi **Azure services v√† setup** kh√°c nhau ƒë·ªÉ ph√π h·ª£p l∆∞·ª£ng ng∆∞·ªùi d√πng v√† t·ªëi ∆∞u cost/latency.

---

## 1. T·ªïng quan theo scale

### AI Application (LLM via API)

| Tier | Request/ng√†y (∆∞·ªõc l∆∞·ª£ng) | Ki·∫øn tr√∫c | Azure services ch√≠nh |
|------|---------------------------|-----------|------------------------|
| **Tier 1** | ~1K ‚Äì 10K | Single region, serverless | Azure OpenAI, Azure AI Search (free/basic), App Service / Functions, Blob, Key Vault |
| **Tier 2** | ~10K ‚Äì 100K | Scale-out + cache | Azure OpenAI, Azure AI Search Standard, App Service / AKS (HPA), Redis Cache, Front Door |
| **Tier 3** | ~100K ‚Äì 500K | AKS + APIM + RAG | Azure OpenAI, Azure AI Search, AKS (orchestration + RAG), Redis Premium, APIM, Service Bus (async) |
| **Tier 4** | ~500K ‚Äì 2M+ | Multi-region, global | Azure OpenAI multi-region, Azure AI Search, AKS multi-region, Front Door Premium (WAF), Cosmos DB (vector/cache), APIM Premium |

### AI Machine Learning / Deep Learning

| Tier | Request/ng√†y (∆∞·ªõc l∆∞·ª£ng) | Ki·∫øn tr√∫c | Azure services ch√≠nh |
|------|---------------------------|-----------|------------------------|
| **Tier 1** | ~1K ‚Äì 10K | Single endpoint, batch ∆∞u ti√™n | Azure ML (batch inference + online 1 instance), Blob / Data Lake, Key Vault |
| **Tier 2** | ~10K ‚Äì 100K | Online inference scale-out | Azure ML Online Endpoint (autoscale), Redis (feature cache n·∫øu c·∫ßn), Front Door |
| **Tier 3** | ~100K ‚Äì 500K | AKS + Azure ML + Feature Store | Azure ML (training + batch), AKS (custom serving ho·∫∑c Azure ML), Redis Premium, Event Hubs / Service Bus |
| **Tier 4** | ~500K ‚Äì 2M+ | Multi-region ML, HA | Azure ML multi-region endpoint, AKS multi-region (Triton/KServe), Cosmos DB / SQL (features), Front Door Premium |

---

## 2. AI Application (LLM) ‚Äì Chi ti·∫øt t·ª´ng tier

### Tier 1: ~1K ‚Äì 10K request/ng√†y (AI Application)

**Use case:** Chatbot n·ªôi b·ªô, demo RAG, POC; v√†i ngh√¨n request/ng√†y.

```mermaid
graph TB
    User[üë§ User]
    FD[Azure Front Door<br/>optional]
    API[App Service B1<br/>ho·∫∑c Functions]
    AO[Azure OpenAI<br/>Pay-per-use]
    Search[Azure AI Search<br/>Free / Basic]
    Blob[Blob Storage]
    KV[Key Vault]
    
    User --> FD --> API
    API --> AO
    API --> Search
    API --> Blob
    API --> KV
```

| Service | Setup |
|---------|--------|
| **Azure OpenAI** | 1 deployment (e.g. gpt-4o-mini); pay-per-token; kh√¥ng c·∫ßn reserved capacity. |
| **Azure AI Search** | Free (3 index) ho·∫∑c Basic; 1 index cho RAG; vector search n·∫øu c·∫ßn. |
| **App Service** | B1, 1 instance; ch·∫°y orchestration (LangChain/Semantic Kernel) + RAG. Ho·∫∑c **Functions** Consumption cho API. |
| **Blob** | Document store cho RAG; indexing v√†o Search. |
| **Key Vault** | API key OpenAI, connection string Search. |

**T·ªëi ∆∞u:** Prompt ng·∫Øn; cache response theo semantic key (Redis optional); gi·ªõi h·∫°n token/request.

---

### Tier 2: ~10K ‚Äì 100K request/ng√†y (AI Application)

**Use case:** Chatbot/RAG production nh·ªè; c·∫ßn autoscale, cache, CDN.

```mermaid
graph TB
    User[üë§ Users]
    FD[Azure Front Door]
    API[App Service P1v2<br/>2‚Äì5 instances<br/>autoscale]
    AO[Azure OpenAI]
    Search[Azure AI Search<br/>Standard]
    Redis[Redis Cache<br/>Standard C1]
    Blob[Blob]
    KV[Key Vault]
    
    User --> FD --> API
    API --> Redis
    API --> AO
    API --> Search
    API --> Blob
    API --> KV
```

| Service | Setup |
|---------|--------|
| **App Service** | P1v2; **autoscale** 2‚Äì5 instance (CPU ho·∫∑c HTTP queue); orchestration + RAG. |
| **Azure OpenAI** | C√≥ th·ªÉ th√™m deployment (gpt-4o) cho route n·∫∑ng; rate limit ph√≠a app. |
| **Azure AI Search** | **Standard** (S1); index RAG + vector; shared private link n·∫øu c·∫ßn. |
| **Redis** | Standard C1; cache embedding, cache response (semantic/key); TTL 5‚Äì60 ph√∫t. |
| **Front Door** | Standard; CDN cho static; route /api t·ªõi App Service. |

**T·ªëi ∆∞u:** Semantic cache (t∆∞∆°ng t·ª± query ‚Üí cache); streaming response; gi·ªõi h·∫°n context length ƒë·ªÉ gi·∫£m cost.

---

### Tier 3: ~100K ‚Äì 500K request/ng√†y (AI Application)

**Use case:** RAG/agent production; nhi·ªÅu tenant ho·∫∑c nhi·ªÅu use case; async job (ingest document, batch embed).

```mermaid
graph TB
    User[üë§ Users]
    FD[Azure Front Door]
    APIM[API Management<br/>Standard]
    AKS[AKS<br/>Orchestration + RAG API<br/>HPA 3‚Äì15]
    AO[Azure OpenAI]
    Search[Azure AI Search<br/>Standard]
    Redis[Redis Premium P1]
    SB[Service Bus<br/>Document ingest queue]
    Blob[Blob]
    
    User --> FD --> APIM --> AKS
    AKS --> Redis
    AKS --> AO
    AKS --> Search
    AKS --> Blob
    AKS --> SB
```

| Service | Setup |
|---------|--------|
| **AKS** | 3‚Äì10 nodes; deployment orchestration + RAG; **HPA** theo CPU ho·∫∑c request/sec (min 3, max 15). |
| **API Management** | Standard; rate limit, versioning, cache; backend = AKS Ingress. |
| **Azure OpenAI** | Nhi·ªÅu deployment (model, region) n·∫øu c·∫ßn; retry + circuit breaker trong app. |
| **Azure AI Search** | Standard (S1/S2); nhi·ªÅu index (tenant ho·∫∑c use case); vector + hybrid. |
| **Redis Premium** | P1; cache embedding + response; persistence n·∫øu c·∫ßn. |
| **Service Bus** | Standard; queue document ingest ‚Üí Function/worker embed ‚Üí update Search. |

**T·ªëi ∆∞u:** T√°ch service: API sync (RAG/chat) vs worker async (ingest); PDB cho API; observability (trace RAG step, token cost).

---

### Tier 4: ~500K ‚Äì 2M+ request/ng√†y (AI Application)

**Use case:** Global AI app; multi-region; WAF; 99.99%; compliance.

```mermaid
graph TB
    subgraph "Users"
        US[üåé US]
        EU[üåç EU]
    end
    AFD[Azure Front Door<br/>Premium ‚Äì WAF]
    subgraph "Region US"
        AKS1[AKS<br/>Orchestration + RAG]
        AO1[Azure OpenAI<br/>US]
        Search1[Azure AI Search]
        Redis1[Redis Premium P3]
    end
    subgraph "Region EU"
        AKS2[AKS]
        AO2[Azure OpenAI<br/>EU]
        Search2[Azure AI Search]
        Redis2[Redis Premium P3]
    end
    Cosmos[(Cosmos DB<br/>Vector / metadata<br/>multi-region)]
    APIM[APIM Premium<br/>multi-region]
    
    US --> AFD
    EU --> AFD
    AFD --> APIM
    APIM --> AKS1
    APIM --> AKS2
    AKS1 --> AO1
    AKS2 --> AO2
    AKS1 --> Search1
    AKS2 --> Search2
    AKS1 --> Redis1
    AKS2 --> Redis2
    AKS1 --> Cosmos
    AKS2 --> Cosmos
```

| Service | Setup |
|---------|--------|
| **Front Door Premium** | WAF (OWASP); bot protection; geo routing ‚Üí APIM g·∫ßn user. |
| **APIM Premium** | Multi-region (US + EU); rate limit, cache, auth. |
| **AKS** | 2+ region; m·ªói region 3‚Äì15 nodes; HPA; Availability Zones. |
| **Azure OpenAI** | Deployment per region (latency); fallback region khi l·ªói. |
| **Azure AI Search** | Standard/Premium; c√≥ th·ªÉ multi-region (replica) n·∫øu d√πng Premium. |
| **Cosmos DB** | Multi-region write; container vector (embedding) ho·∫∑c metadata; partition key theo tenant/region. |
| **Redis** | Premium P3 m·ªói region; cache kh√¥ng replicate (local). |

**T·ªëi ∆∞u:** Health probe + failover; Content Safety / filter per region; audit log; cost (token + embedding) theo region.

---

## 3. AI Machine Learning / Deep Learning ‚Äì Chi ti·∫øt t·ª´ng tier

### Tier 1: ~1K ‚Äì 10K request/ng√†y (ML/DL)

**Use case:** Internal tool, batch scoring l√† ch√≠nh; online inference √≠t.

```mermaid
graph TB
    Data[Raw Data]
    AML[Azure ML<br/>Workspace]
    subgraph "Training"
        Train[Pipeline<br/>Training]
        MR[Model Registry]
    end
    subgraph "Inference"
        Batch[Batch Endpoint<br/>schedule]
        Online[Online Endpoint<br/>1 instance]
    end
    Blob[Blob / Data Lake]
    KV[Key Vault]
    
    Data --> Blob
    Blob --> Train
    Train --> MR
    MR --> Batch
    MR --> Online
    AML --> KV
```

| Service | Setup |
|---------|--------|
| **Azure ML** | Workspace; **Batch Endpoint** (schedule hourly/daily); **Online Endpoint** 1 instance (e.g. Standard_DS2_v2). |
| **Blob / Data Lake** | Raw data, training output; input/output cho batch. |
| **Model Registry** | Version model; promote staging ‚Üí prod. |

**T·ªëi ∆∞u:** ∆Øu ti√™n batch; online ch·ªâ khi c·∫ßn real-time; instance nh·ªè, scale-to-zero n·∫øu d√πng serverless option.

---

### Tier 2: ~10K ‚Äì 100K request/ng√†y (ML/DL)

**Use case:** Online inference tƒÉng; c·∫ßn autoscale, latency ·ªïn ƒë·ªãnh.

```mermaid
graph TB
    User[üë§ Clients]
    FD[Azure Front Door]
    AML[Azure ML<br/>Online Endpoint]
    subgraph "Endpoint"
        Autoscale[Autoscale 2‚Äì10<br/>CPU / request]
    end
    Redis[Redis Cache<br/>Feature / response cache]
    Blob[Blob]
    
    User --> FD --> AML
    AML --> Redis
    AML --> Blob
```

| Service | Setup |
|---------|--------|
| **Azure ML Online Endpoint** | **Autoscale** 2‚Äì10 instance (CPU 70% ho·∫∑c request count); deployment v·ªõi model + scoring script. |
| **Redis** | Cache feature (n·∫øu d√πng Feature Store offline) ho·∫∑c cache response cho same input. |
| **Front Door** | Single entry; health probe; optional CDN. |

**T·ªëi ∆∞u:** Resource request/limit ƒë√∫ng; probe readiness; cache key = hash(features) ƒë·ªÉ gi·∫£m latency v√† cost.

---

### Tier 3: ~100K ‚Äì 500K request/ng√†y (ML/DL)

**Use case:** Nhi·ªÅu model, batch + online; c√≥ th·ªÉ custom serving (Triton, KServe) tr√™n AKS.

```mermaid
graph TB
    User[üë§ Clients]
    FD[Azure Front Door]
    APIM[API Management]
    subgraph "AKS"
        Serving[Model Serving<br/>Triton / KServe<br/>HPA]
    end
    AML[Azure ML<br/>Training + Batch]
    Redis[Redis Premium<br/>Feature cache]
    EH[Event Hubs / Service Bus<br/>Trigger retrain]
    Blob[Blob]
    
    User --> FD --> APIM --> Serving
    Serving --> Redis
    AML --> Serving
    AML --> Blob
    Blob --> EH --> AML
```

| Service | Setup |
|---------|--------|
| **AKS** | Node pool cho inference (GPU n·∫øu c·∫ßn); **Triton** ho·∫∑c **KServe**; HPA theo request. |
| **Azure ML** | Training pipeline (schedule/data trigger); Batch Endpoint; model deploy l√™n AKS ho·∫∑c Azure ML Managed Online. |
| **Redis Premium** | Feature cache (online store) n·∫øu d√πng Feature Store pattern. |
| **Event Hubs / Service Bus** | Event trigger retraining khi data m·ªõi. |
| **APIM** | Rate limit, versioning (model v1/v2). |

**T·ªëi ∆∞u:** T√°ch node pool (GPU vs CPU); PDB cho serving; monitoring drift + latency.

---

### Tier 4: ~500K ‚Äì 2M+ request/ng√†y (ML/DL)

**Use case:** Global ML; multi-region inference; HA; compliance.

```mermaid
graph TB
    subgraph "Users"
        US[üåé US]
        EU[üåç EU]
    end
    AFD[Azure Front Door<br/>Premium]
    subgraph "Region US"
        AKS1[AKS<br/>Model Serving]
        AML1[Azure ML<br/>US]
        Redis1[Redis P3]
    end
    subgraph "Region EU"
        AKS2[AKS<br/>Model Serving]
        AML2[Azure ML<br/>EU]
        Redis2[Redis P3]
    end
    Cosmos[(Cosmos DB<br/>Features / metadata<br/>multi-region)]
    
    US --> AFD
    EU --> AFD
    AFD --> AKS1
    AFD --> AKS2
    AKS1 --> AML1
    AKS2 --> AML2
    AKS1 --> Redis1
    AKS2 --> Redis2
    AKS1 --> Cosmos
    AKS2 --> Cosmos
```

| Service | Setup |
|---------|--------|
| **Front Door Premium** | Geo routing; WAF; health probe; failover. |
| **AKS** | 2+ region; model serving (Triton/KServe ho·∫∑c Azure ML Managed); Availability Zones. |
| **Azure ML** | Workspace per region (ho·∫∑c 1 global + endpoint multi-region); training t·∫°i region g·∫ßn data n·∫øu compliance. |
| **Cosmos DB** | Feature store metadata ho·∫∑c feature cache multi-region (n·∫øu c·∫ßn). |
| **Redis** | Premium P3 m·ªói region; feature/response cache. |

**T·ªëi ∆∞u:** Data residency (training + inference t·∫°i region ƒë√∫ng); model versioning + rollback; SLA monitoring.

---

## 4. B·∫£ng ch·ªçn nhanh

| Nhu c·∫ßu | Nh√°nh | Tier g·ª£i √Ω |
|---------|--------|------------|
| Chatbot/RAG demo, POC | AI Application | Tier 1 |
| RAG/agent production, 10K‚Äì100K req/ng√†y | AI Application | Tier 2 |
| RAG global, multi-tenant, 500K+ | AI Application | Tier 4 |
| Batch scoring l√† ch√≠nh, √≠t online | AI ML/DL | Tier 1 |
| Online inference 10K‚Äì100K, autoscale | AI ML/DL | Tier 2 |
| Nhi·ªÅu model, AKS custom serving | AI ML/DL | Tier 3 |
| ML global, HA, compliance | AI ML/DL | Tier 4 |

---

## 5. T√†i li·ªáu tham kh·∫£o

- [Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/)
- [Azure AI Search](https://learn.microsoft.com/en-us/azure/search/)
- [Azure ML ‚Äì Endpoints](https://learn.microsoft.com/en-us/azure/machine-learning/concept-endpoints)
- [Azure ML ‚Äì Batch inference](https://learn.microsoft.com/en-us/azure/machine-learning/concept-batch-inference)
- [Azure Front Door](https://learn.microsoft.com/en-us/azure/frontdoor/)
- [AKS Best practices](https://learn.microsoft.com/en-us/azure/aks/best-practices)
